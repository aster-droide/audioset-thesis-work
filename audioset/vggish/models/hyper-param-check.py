# -*- coding: utf-8 -*-
"""11mar-stratified-hyperparam-augmented-data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q2Xi9PkztlZ-9xhruesZdNoCh0Oxirl-
"""

import pandas as pd
import numpy as np
import random
import keras
import keras_tuner as kt
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import BatchNormalization
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input, concatenate
from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from tensorflow.keras.models import Model
from sklearn.inspection import permutation_importance
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GroupKFold
from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping
import os

# Force TensorFlow to use CPU
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'

# Verify that TensorFlow does not see the GPU (should print an empty list)
print("Available GPU devices: ", tf.config.list_physical_devices('GPU'))


# Set a fixed random seed for reproducibility
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

# Load the dataset from the CSV file
dataframe = pd.read_csv('/Users/astrid/PycharmProjects/tensorflow-fork/research/audioset/vggish/all_embeddings_11Mar.csv')

augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/tensorflow-fork/research/audioset/vggish/Aug_embed_march_11.csv')

def assign_age_group(age, age_groups):
    for group_name, age_range in age_groups.items():
        if age_range[0] <= age < age_range[1]:
            return group_name
    return 'Unknown'  # For any age that doesn't fit the defined groups

# Define your age groups
age_groups = {
    'kitten': (0, 1),
    'adult': (1, 12),
    'senior': (12, 19)
}

# Apply the function to create a new column for the age group
dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)

# Apply the function to create a new column for the age group
augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)

# Assuming 'cat_id' identifies each cat and 'age_group' has been assigned
meow_counts = dataframe.groupby(['cat_id', 'age_group']).size().reset_index(name='counts')

dataframe['age_group'].value_counts()

augmented_df['age_group'].value_counts()


"""# CAT OCCURS ONLY ONCE IN EACH SET"""

# Correctly aggregate to find a single label for each cat_id
cat_id_labels = dataframe.groupby('cat_id')['age_group'].agg(lambda x: x.mode()[0]).reset_index()

# Encode the labels for stratification
label_encoder = LabelEncoder()
cat_id_labels['target_encoded'] = label_encoder.fit_transform(cat_id_labels['age_group'])

# Split cat_ids into train, test, and then train into train, validation, with stratification
train_ids, test_ids = train_test_split(cat_id_labels, stratify=cat_id_labels['target_encoded'], test_size=0.2, random_state=42)

# Define a function to print the distribution of labels along with each identifier and percentage of labels
def print_label_distribution_and_percentages(ids_dataframe, label_encoder):
    # Decode the labels back to their original form for readability
    ids_dataframe['label'] = label_encoder.inverse_transform(ids_dataframe['target_encoded'])
    print("Identifier and Label Distribution:")
    for index, row in ids_dataframe.iterrows():
        print(f"Identifier: {row['cat_id']}, Label: {row['label']}")

    # Print the percentage of labels in the set
    label_counts = ids_dataframe['label'].value_counts()
    total_counts = label_counts.sum()
    print("\nLabel Percentages in This Set:")
    for label, count in label_counts.items():
        percentage = (count / total_counts) * 100
        print(f"Label: {label}, Count: {count} Percentage: {percentage:.2f}%")

# Print the distribution and label percentages for the training set
print("Training Set:")
print_label_distribution_and_percentages(train_ids, label_encoder)

# Print the distribution and label percentages for the test set
print("\nTest Set:")
print_label_distribution_and_percentages(test_ids, label_encoder)

# Define function to filter df by cat_ids
def filter_by_cat_ids(df, ids):
    return df[df['cat_id'].isin(ids['cat_id'])]

# Apply filtering
# what am i doing here?
train_df = filter_by_cat_ids(dataframe, train_ids)
test_df = filter_by_cat_ids(dataframe, test_ids)

def print_label_percentages(df, label_encoder):
    # Ensure 'age_group' column is present
    if 'age_group' not in df.columns:
        raise KeyError("The dataframe does not contain a 'age_group' column.")

    # Encode the 'age_group' labels
    labels_encoded = label_encoder.transform(df['age_group'])
    labels = label_encoder.inverse_transform(labels_encoded)

    # Count label occurrences and calculate percentages
    labels_series = pd.Series(labels)
    label_counts = labels_series.value_counts()
    total_counts = label_counts.sum()
    print("\nLabel Percentages:")
    for label, count in label_counts.items():
        percentage = (count / total_counts) * 100
        print(f"Label: {label}, Count: {count} Percentage: {percentage:.2f}%")

# Assuming you have already defined label_encoder with LabelEncoder()
# and it's fitted with your dataset labels

print("Training Set:")
print_label_percentages(train_df, label_encoder)

print("\nTest Set:")
print_label_percentages(test_df, label_encoder)

"""# ^ SHOULD MAYBE JUST GRAB SOME EXRTRA TEST SAMPELS FROM TRAIN FOR THE MINORITY CLASSES"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adamax
import keras_tuner as kt

def build_model(hp):
    model = Sequential()
    model.add(Dense(
        units=hp.Int('units_1', min_value=32, max_value=512, step=32),
        activation='relu',
        input_shape=(X_train_scaled.shape[1],)
    ))
    model.add(Dropout(hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)))

    # Conditional addition of a second layer
    if hp.Boolean('use_second_layer'):
        model.add(Dense(
            units=hp.Int('units_2', min_value=32, max_value=512, step=32),
            activation=hp.Choice('activation_2', values=['relu', 'tanh', 'sigmoid'])),
        )
        model.add(Dropout(hp.Float('dropout_2', min_value=0.0, max_value=0.5, step=0.1)))

    model.add(Dense(3, activation='softmax'))

    model.compile(
        optimizer=keras.optimizers.legacy.Adamax(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

# The second last column is the cat identifier
cat_ids = dataframe.iloc[:, -2].values

# Separate features and labels
X = dataframe.iloc[:, :-3].values  # all columns except the last three
y = dataframe.iloc[:, -1].values   # the last column (target)

# Encode the class labels to integers
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)
# Convert integers to dummy variables (i.e. one hot encoded)
y_dummy = to_categorical(y_encoded)

# Split the dataset while keeping cats together
unique_cat_ids = np.unique(cat_ids)

# Initial split: Separate data into training+validation and test sets
train_val_cats, test_cats = train_test_split(unique_cat_ids, test_size=0.2, random_state=42)

# Further split the training+validation set into separate training and validation sets
train_cats, val_cats = train_test_split(train_val_cats, test_size=0.25, random_state=42)  # Adjust the test_size as necessary

# Now you have three sets of cat identifiers: train_cats, val_cats, test_cats

# Filter augmented data to include only those that match the train_cats
augmented_train_df = augmented_df[augmented_df['cat_id'].isin(train_cats)]

# Assuming augmented_train_df now contains the correctly matched augmented data
# Prepare features and labels from augmented data
X_augmented = augmented_train_df.iloc[:, :-3].values  # Adjust indices as necessary
y_augmented = augmented_train_df.iloc[:, -1].values  # Last column for target, adjust as necessary

# Encode and one-hot encode y_augmented if necessary (assuming encoder is fitted on original y)
y_augmented_encoded = encoder.transform(y_augmented)
y_augmented_dummy = to_categorical(y_augmented_encoded)

# Function to get indices for cat groups
def get_indices(cat_group, all_cats):
    return [i for i, cat in enumerate(all_cats) if cat in cat_group]

train_indices = get_indices(train_cats, cat_ids)
test_indices = get_indices(test_cats, cat_ids)

len(test_indices)

# Extract identifiers for each set to confirm uniqueness
train_identifiers = np.unique(cat_ids[train_indices])
test_identifiers = np.unique(cat_ids[test_indices])

# Confirm uniqueness
assert len(set(train_identifiers) & set(test_identifiers)) == 0

# Print identifiers for confirmation
print("Train Identifiers:", train_identifiers)
print("Test Identifiers:", test_identifiers)

# Use get_indices function to get indices for train, validation, and test sets
train_indices = get_indices(train_cats, cat_ids)
val_indices = get_indices(val_cats, cat_ids)
test_indices = get_indices(test_cats, cat_ids)

# Prepare X and y for train, validation, and test sets
X_train, y_train = X[train_indices], y_dummy[train_indices]
X_val, y_val = X[val_indices], y_dummy[val_indices]
X_test, y_test = X[test_indices], y_dummy[test_indices]

# For augmented data, ensure it's added only to the training set
X_train_combined = np.vstack((X_train, X_augmented))
y_train_combined = np.vstack((y_train, y_augmented_dummy))

# Normalize the features using Min-Max scaling based on the training set
# TODO: always check results with different scalers
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_combined)  # Fit scaler to training data
X_val_scaled = scaler.transform(X_val)  # Transform validation data
X_test_scaled = scaler.transform(X_test)  # Transform test data

# Check if the number of samples matches the number of labels
if X_train_scaled.shape[0] != y_train_combined.shape[0]:
    raise ValueError("The number of samples in X_train_scaled does not match the number of labels in y_train_combined.")
else:
    print("The dimensions of X_train_scaled and y_train_combined align correctly. Proceed with model training.")

tuner = kt.RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=500,
    executions_per_trial=1,
    directory='keras_tuner_dir',
    project_name='hyperparam_opt'
)

# Define callbacks for the tuning process
csv_logger = CSVLogger('training_logs.csv', append=True)

# This will save the best model across all tuning trials
model_checkpoint = ModelCheckpoint(
    'best_model.h5',
    save_best_only=True,
    monitor='val_accuracy',
    mode='max'
)

# EarlyStopping to prevent overfitting and to stop early if no improvement
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=100,
    restore_best_weights=True,
    verbose=1
)

# Start the hyperparameter search
with tf.device('/CPU:0'):
    tuner.search(
        X_train_scaled, y_train_combined,
        epochs=200,
        validation_data=(X_val_scaled, y_val),
        callbacks=[csv_logger, model_checkpoint, early_stopping]
    )

# Get the hyperparameters of the best model
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

# Print out the best hyperparameters
for hp in best_hps.space:
    print(f'{hp.name}: {best_hps.get(hp.name)}')

# Print a summary of the best model
best_model = tuner.get_best_models(num_models=1)[0]
best_model.summary()












